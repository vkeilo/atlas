{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sse_starlette.sse import EventSourceResponse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, base_model=None, lora_model=None, tokenizer_path=None, gpus=\"0\",\n",
    "                 load_in_8bit=False, load_in_4bit=False, only_cpu=False, alpha=\"1.0\"):\n",
    "        self.base_model = base_model\n",
    "        self.lora_model = lora_model\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.gpus = gpus\n",
    "        self.load_in_8bit = load_in_8bit\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.only_cpu = only_cpu\n",
    "        self.alpha = alpha\n",
    "\n",
    "# 在这里设置你的参数\n",
    "args = Args(\n",
    "    base_model=\"/media/vkeilo/game/github_project/model_datas/chinese-alpaca-2-7b-hf\",\n",
    "    lora_model=None,  # 或保持为None，根据你的需要\n",
    "    tokenizer_path=None,\n",
    "    gpus=\"0,1\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    "    only_cpu=True,\n",
    "    alpha=\"1.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vkeilo/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if args.only_cpu is True:\n",
    "    args.gpus = \"\"\n",
    "    if args.load_in_8bit or args.load_in_4bit:\n",
    "        raise ValueError(\"Quantization is unavailable on CPU.\")\n",
    "if args.load_in_8bit and args.load_in_4bit:\n",
    "    raise ValueError(\"Only one quantization method can be chosen for inference. Please check your arguments\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    GenerationConfig,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_api_server.py 的父目录\n",
    "sys.path.append('/media/vkeilo/game/github_project/Chinese-LLaMA-Alpaca-2/scripts')\n",
    "sys.path.append('/media/vkeilo/game/github_project/Chinese-LLaMA-Alpaca-2/scripts/openai_server_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_MEM_EFF_ATTENTION:  True\n",
      "STORE_KV_BEFORE_ROPE: False\n",
      "Apply NTK scaling with ALPHA=1.0\n",
      "The value of scaling factor will be read from model config file, or set to 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "from attn_and_long_ctx_patches import apply_attention_patch, apply_ntk_scaling_patch\n",
    "\n",
    "apply_attention_patch(use_memory_efficient_attention=True)\n",
    "apply_ntk_scaling_patch(args.alpha)\n",
    "\n",
    "from openai_api_protocol import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatMessage,\n",
    "    ChatCompletionResponseChoice,\n",
    "    CompletionRequest,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseChoice,\n",
    "    EmbeddingsRequest,\n",
    "    EmbeddingsResponse,\n",
    "    ChatCompletionResponseStreamChoice,\n",
    "    DeltaMessage,\n",
    ")\n",
    "\n",
    "load_type = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "if args.tokenizer_path is None:\n",
    "    args.tokenizer_path = args.lora_model\n",
    "    if args.lora_model is None:\n",
    "        args.tokenizer_path = args.base_model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(args.tokenizer_path, legacy=True)\n",
    "if args.load_in_4bit or args.load_in_8bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        bnb_4bit_compute_dtype=load_type,\n",
    "    )\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    args.base_model,\n",
    "    torch_dtype=load_type,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto' if not args.only_cpu else None,\n",
    "    load_in_4bit=args.load_in_4bit,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    quantization_config=quantization_config if (args.load_in_4bit or args.load_in_8bit) else None\n",
    "    # vkeilo add it\n",
    "    # output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看base模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(55296, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=55296, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以k矩阵为例，所取出的128*4096对应 k向量维度*hidden_size 与attention论文中矩阵中的k矩阵为转置关系\n",
    "# o矩阵不同，所取出的为 hidden_size*（32*128）的矩阵\n",
    "k_matrixes = torch.empty(0)\n",
    "q_matrixes = torch.empty(0)\n",
    "v_matrixes = torch.empty(0)\n",
    "o_matrixes = torch.empty(0)\n",
    "for layer in base_model.model.layers:\n",
    "    tmp_k_matrixes = layer.self_attn.k_proj.weight.data\n",
    "    tmp_q_matrixes = layer.self_attn.q_proj.weight.data\n",
    "    tmp_v_matrixes = layer.self_attn.v_proj.weight.data\n",
    "    tmp_o_matrixes = layer.self_attn.o_proj.weight.data\n",
    "    tmp_k_matrixes = tmp_k_matrixes.reshape(1,32,128,4096)\n",
    "    tmp_q_matrixes = tmp_q_matrixes.reshape(1,32,128,4096)\n",
    "    tmp_v_matrixes = tmp_v_matrixes.reshape(1,32,128,4096)\n",
    "    tmp_o_matrixes = tmp_o_matrixes.reshape(1,4096,32,128).permute(0, 2, 1, 3)\n",
    "    k_matrixes = torch.cat((k_matrixes, tmp_k_matrixes), dim=0)\n",
    "    q_matrixes = torch.cat((q_matrixes, tmp_q_matrixes), dim=0)\n",
    "    v_matrixes = torch.cat((v_matrixes, tmp_v_matrixes), dim=0)\n",
    "    o_matrixes = torch.cat((o_matrixes, tmp_o_matrixes), dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在论文《Attention is all you need》中，$score = qk^T$的前提是$q_1 = x_1W_Q$,\n",
    "而在论文《A Mathematical Framework for Transformer Circuits》中，$w_{QK}=W_Q^TW_K$是因为$k_i=W_Kx_i$\n",
    "两篇论文的k,q,v矩阵互为转置关系，代码以后者为基准。\n",
    "W_Q矩阵的行数为128（中间向量维度），列数为4096（hidden_size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "层:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 0 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 1 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 2 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 3 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 4 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 5 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 6 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 7 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 8 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 9 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 10 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 11 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 12 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 13 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 14 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 15 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 16 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 17 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 18 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 19 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 20 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 21 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 22 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 23 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 24 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 25 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 26 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 27 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 28 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 29 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 30 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "层:   3%|▎         | 1/32 [05:33<2:52:05, 333.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 31 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "层:   3%|▎         | 1/32 [05:43<2:57:19, 343.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m q_k_product \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q_matrix\u001b[39m.\u001b[39mt(), k_matrix)\n\u001b[1;32m     22\u001b[0m \u001b[39m# 计算特征值和特征向量\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m eigenvalues, eigenvectors \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meig(q_k_product)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m{\u001b[39;00mlayer_idx\u001b[39m}\u001b[39;00m\u001b[39m head \u001b[39m\u001b[39m{\u001b[39;00mhead_idx\u001b[39m}\u001b[39;00m\u001b[39m processed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39m# 填充列表,保存在内存上\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 构建32*32的列表，对应32层下的32个注意力头\n",
    "attention_heads_list = np.array([[{} for _ in range(32)] for _ in range(32)])\n",
    "\n",
    "# 遍历每一层和每个注意力头\n",
    "for layer_idx in tqdm(range(32), desc='层'):\n",
    "    for head_idx in tqdm(range(32), desc='头', leave=False):\n",
    "        # 提取对应层和头的q和k矩阵\n",
    "        q_matrix = q_matrixes[layer_idx, head_idx, :, :].reshape(128, 4096).to(device)\n",
    "        k_matrix = k_matrixes[layer_idx, head_idx, :, :].reshape(128, 4096).to(device)\n",
    "        \n",
    "        # 将numpy数组转换为torch张量，并将其移动到GPU上\n",
    "        # q_matrix = q_matrix.clone().detach()\n",
    "        # k_matrix = k_matrix.clone().detach()\n",
    "        \n",
    "        # 计算q^Tk，注意这里的q和k都是已经reshape过的，所以需要转置k来进行矩阵乘法\n",
    "        q_k_product = torch.matmul(q_matrix.t(), k_matrix)\n",
    "        \n",
    "        # 计算特征值和特征向量\n",
    "        eigenvalues, eigenvectors = torch.linalg.eig(q_k_product)\n",
    "        print(f'layer{layer_idx} head {head_idx} processed')\n",
    "        # 填充列表,保存在内存上\n",
    "        attention_heads_list[layer_idx][head_idx] = {\n",
    "            '特征值': eigenvalues.to('cpu').detach().numpy(),\n",
    "            '特征向量': eigenvectors.permute(1,0).to('cpu').detach().numpy()\n",
    "        }\n",
    "        # 清理不需要的显存占用\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存array变量attention_heads_list到文件中\n",
    "with open('datas/QK_array.npy', 'wb') as f:\n",
    "    np.save(f, attention_heads_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_q_matrix = q_matrixes[0, 0, :, :].reshape(128, 4096)\n",
    "tmp_k_matrix = k_matrixes[0, 0, :, :].reshape(128, 4096)\n",
    "tq = tmp_q_matrix.clone().detach()\n",
    "tk = tmp_q_matrix.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.matmul(tq.t(),tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta,tb = torch.linalg.eig(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 取第一个特征值和特征向量\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m first_eigenvalue \u001b[39m=\u001b[39m ta[:, \u001b[39m0\u001b[39;49m]\n\u001b[1;32m      3\u001b[0m first_eigenvector \u001b[39m=\u001b[39m tb[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39m# 验证 Ax = λx\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# 取第一个特征值和特征向量\n",
    "first_eigenvalue = ta[:, 0]\n",
    "first_eigenvector = tb[:, 0]\n",
    "\n",
    "# 验证 Ax = λx\n",
    "Ax = torch.matmul(mat, first_eigenvector)\n",
    "Ax_over_lambda = Ax / first_eigenvalue\n",
    "\n",
    "# 打印验证结果\n",
    "print(\"验证结果:\", torch.allclose(Ax_over_lambda, first_eigenvector))\n",
    "\n",
    "# 打印特征值和特征向量\n",
    "print(\"第一个特征值:\", first_eigenvalue)\n",
    "print(\"第一个特征向量:\", first_eigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = ta[0]*tb[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0082+0.j, -0.0201+0.j,  0.0175+0.j,  ...,  0.0108+0.j,  0.0203+0.j,\n",
       "        -0.0167+0.j])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-14.1930+0.j, -34.7486+0.j,  30.3795+0.j,  ...,  18.7687+0.j,  35.1911+0.j,\n",
       "        -28.8638+0.j])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-14.1924, -34.7503,  30.3795,  ...,  18.7687,  35.1912, -28.8638])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = torch.matmul(mat,tb[:,0].real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "print(mat.shape)\n",
    "print(tb[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
