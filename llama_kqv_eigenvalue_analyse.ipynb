{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sse_starlette.sse import EventSourceResponse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, base_model=None, lora_model=None, tokenizer_path=None, gpus=\"0\",\n",
    "                 load_in_8bit=False, load_in_4bit=False, only_cpu=False, alpha=\"1.0\"):\n",
    "        self.base_model = base_model\n",
    "        self.lora_model = lora_model\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.gpus = gpus\n",
    "        self.load_in_8bit = load_in_8bit\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.only_cpu = only_cpu\n",
    "        self.alpha = alpha\n",
    "\n",
    "# 在这里设置你的参数\n",
    "args = Args(\n",
    "    base_model=\"../model_datas/chinese-alpaca-2-7b-hf\",\n",
    "    # base_model=\"/media/vkeilo/game/github_project/model_datas/chinese-alpaca-2-7b-hf\",\n",
    "    lora_model=None,  # 或保持为None，根据你的需要\n",
    "    tokenizer_path=None,\n",
    "    gpus=\"0,1\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    "    only_cpu=True,\n",
    "    alpha=\"1.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\chatglm2-6b\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if args.only_cpu is True:\n",
    "    args.gpus = \"\"\n",
    "    if args.load_in_8bit or args.load_in_4bit:\n",
    "        raise ValueError(\"Quantization is unavailable on CPU.\")\n",
    "if args.load_in_8bit and args.load_in_4bit:\n",
    "    raise ValueError(\"Only one quantization method can be chosen for inference. Please check your arguments\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    GenerationConfig,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_api_server.py 的父目录\n",
    "sys.path.append('/media/vkeilo/game/github_project/Chinese-LLaMA-Alpaca-2/scripts')\n",
    "sys.path.append('/media/vkeilo/game/github_project/Chinese-LLaMA-Alpaca-2/scripts/openai_server_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../Chinese-LLaMA-Alpaca-2/scripts')\n",
    "sys.path.append('../Chinese-LLaMA-Alpaca-2/scripts/openai_server_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention use the following command to install Xformers\n",
      "pip install xformers.\n",
      "USE_MEM_EFF_ATTENTION:  False\n",
      "STORE_KV_BEFORE_ROPE: False\n",
      "Apply NTK scaling with ALPHA=1.0\n",
      "The value of scaling factor will be read from model config file, or set to 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from attn_and_long_ctx_patches import apply_attention_patch, apply_ntk_scaling_patch\n",
    "\n",
    "apply_attention_patch(use_memory_efficient_attention=True)\n",
    "apply_ntk_scaling_patch(args.alpha)\n",
    "\n",
    "from openai_api_protocol import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatMessage,\n",
    "    ChatCompletionResponseChoice,\n",
    "    CompletionRequest,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseChoice,\n",
    "    EmbeddingsRequest,\n",
    "    EmbeddingsResponse,\n",
    "    ChatCompletionResponseStreamChoice,\n",
    "    DeltaMessage,\n",
    ")\n",
    "\n",
    "load_type = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "if args.tokenizer_path is None:\n",
    "    args.tokenizer_path = args.lora_model\n",
    "    if args.lora_model is None:\n",
    "        args.tokenizer_path = args.base_model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(args.tokenizer_path, legacy=True)\n",
    "if args.load_in_4bit or args.load_in_8bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        bnb_4bit_compute_dtype=load_type,\n",
    "    )\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    args.base_model,\n",
    "    torch_dtype=load_type,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto' if not args.only_cpu else None,\n",
    "    load_in_4bit=args.load_in_4bit,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    quantization_config=quantization_config if (args.load_in_4bit or args.load_in_8bit) else None\n",
    "    # vkeilo add it\n",
    "    # output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看base模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(55296, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=55296, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_matrix = base_model.model.embed_tokens.weight.data\n",
    "u_matrix = base_model.lm_head.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 55296])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_matrix.t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以k矩阵为例，所取出的128*4096对应 k向量维度*hidden_size 与attention论文中矩阵中的k矩阵为转置关系\n",
    "# o矩阵不同，所取出的为 hidden_size*（32*128）的矩阵\n",
    "k_matrixes = torch.empty(0)\n",
    "q_matrixes = torch.empty(0)\n",
    "v_matrixes = torch.empty(0)\n",
    "o_matrixes = torch.empty(0)\n",
    "for layer in base_model.model.layers:\n",
    "    tmp_k_matrixes = layer.self_attn.k_proj.weight.data\n",
    "    tmp_q_matrixes = layer.self_attn.q_proj.weight.data\n",
    "    tmp_v_matrixes = layer.self_attn.v_proj.weight.data\n",
    "    tmp_o_matrixes = layer.self_attn.o_proj.weight.data\n",
    "    tmp_k_matrixes = tmp_k_matrixes.reshape(1,32,128,4096)\n",
    "    tmp_q_matrixes = tmp_q_matrixes.reshape(1,32,128,4096)\n",
    "    tmp_v_matrixes = tmp_v_matrixes.reshape(1,32,128,4096)\n",
    "    tmp_o_matrixes = tmp_o_matrixes.reshape(1,4096,32,128).permute(0, 2, 1, 3)\n",
    "    k_matrixes = torch.cat((k_matrixes, tmp_k_matrixes), dim=0)\n",
    "    q_matrixes = torch.cat((q_matrixes, tmp_q_matrixes), dim=0)\n",
    "    v_matrixes = torch.cat((v_matrixes, tmp_v_matrixes), dim=0)\n",
    "    o_matrixes = torch.cat((o_matrixes, tmp_o_matrixes), dim=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在论文《Attention is all you need》中，$score = qk^T$的前提是$q_1 = x_1W_Q$,\n",
    "而在论文《A Mathematical Framework for Transformer Circuits》中，$w_{QK}=W_Q^TW_K$是因为$k_i=W_Kx_i$\n",
    "两篇论文的k,q,v矩阵互为转置关系，代码以后者为基准。\n",
    "W_Q矩阵的行数为128（中间向量维度），列数为4096（hidden_size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qk矩阵的特征值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 head 3 processed\n",
      "layer0 head 0 processed\n",
      "layer0 head 2 processed\n",
      "layer0 head 1 processed\n",
      "Processed 1 /1024 tasks\n",
      "Processed 2 /1024 tasks\n",
      "Processed 3 /1024 tasks\n",
      "Processed 4 /1024 tasks\n",
      "layer0 head 4 processed\n",
      "Processed 5 /1024 tasks\n",
      "layer0 head 6 processed\n",
      "layer0 head 7 processed\n",
      "layer0 head 5 processed\n",
      "Processed 6 /1024 tasks\n",
      "Processed 7 /1024 tasks\n",
      "Processed 8 /1024 tasks\n",
      "layer0 head 8 processed\n",
      "Processed 9 /1024 tasks\n",
      "layer0 head 9 processed\n",
      "Processed 10 /1024 tasks\n",
      "layer0 head 10 processed\n",
      "Processed 11 /1024 tasks\n",
      "layer0 head 11 processed\n",
      "Processed 12 /1024 tasks\n",
      "layer0 head 12 processed\n",
      "Processed 13 /1024 tasks\n",
      "layer0 head 13 processed\n",
      "Processed 14 /1024 tasks\n",
      "layer0 head 15 processed\n",
      "layer0 head 14 processed\n",
      "Processed 15 /1024 tasks\n",
      "Processed 16 /1024 tasks\n",
      "layer0 head 16 processed\n",
      "Processed 17 /1024 tasks\n",
      "layer0 head 19 processed\n",
      "layer0 head 17 processed\n",
      "Processed 18 /1024 tasks\n",
      "layer0 head 18 processed\n",
      "Processed 19 /1024 tasks\n",
      "Processed 20 /1024 tasks\n",
      "layer0 head 20 processed\n",
      "Processed 21 /1024 tasks\n",
      "layer0 head 22 processed\n",
      "layer0 head 23 processed\n",
      "layer0 head 21 processed\n",
      "Processed 22 /1024 tasks\n",
      "Processed 23 /1024 tasks\n",
      "Processed 24 /1024 tasks\n",
      "layer0 head 24 processed\n",
      "Processed 25 /1024 tasks\n",
      "layer0 head 25 processed\n",
      "Processed 26 /1024 tasks\n",
      "layer0 head 26 processed\n",
      "Processed 27 /1024 tasks\n",
      "layer0 head 27 processed\n",
      "Processed 28 /1024 tasks\n",
      "layer0 head 28 processed\n",
      "Processed 29 /1024 tasks\n",
      "layer0 head 30 processed\n",
      "layer0 head 29 processed\n",
      "layer0 head 31 processed\n",
      "Processed 30 /1024 tasks\n",
      "Processed 31 /1024 tasks\n",
      "Processed 32 /1024 tasks\n",
      "layer1 head 3 processed\n",
      "layer1 head 2 processed\n",
      "layer1 head 1 processed\n",
      "layer1 head 0 processed\n",
      "Processed 1 /1024 tasks\n",
      "Processed 2 /1024 tasks\n",
      "Processed 3 /1024 tasks\n",
      "Processed 4 /1024 tasks\n",
      "layer1 head 7 processed\n",
      "layer1 head 4 processed\n",
      "Processed 5 /1024 tasks\n",
      "layer1 head 6 processed\n",
      "layer1 head 5 processed\n",
      "Processed 6 /1024 tasks\n",
      "Processed 7 /1024 tasks\n",
      "Processed 8 /1024 tasks\n",
      "layer1 head 9 processed\n",
      "layer1 head 11 processed\n",
      "layer1 head 10 processed\n",
      "layer1 head 8 processed\n",
      "Processed 9 /1024 tasks\n",
      "Processed 10 /1024 tasks\n",
      "Processed 11 /1024 tasks\n",
      "Processed 12 /1024 tasks\n",
      "layer1 head 15 processed\n",
      "layer1 head 13 processed\n",
      "layer1 head 12 processed\n",
      "layer1 head 14 processed\n",
      "Processed 13 /1024 tasks\n",
      "Processed 14 /1024 tasks\n",
      "Processed 15 /1024 tasks\n",
      "Processed 16 /1024 tasks\n",
      "layer1 head 18 processed\n",
      "layer1 head 16 processed\n",
      "Processed 17 /1024 tasks\n",
      "layer1 head 17 processed\n",
      "Processed 18 /1024 tasks\n",
      "Processed 19 /1024 tasks\n",
      "layer1 head 19 processed\n",
      "Processed 20 /1024 tasks\n",
      "layer1 head 21 processed\n",
      "layer1 head 20 processed\n",
      "Processed 21 /1024 tasks\n",
      "Processed 22 /1024 tasks\n",
      "layer1 head 23 processed\n",
      "layer1 head 22 processed\n",
      "Processed 23 /1024 tasks\n",
      "Processed 24 /1024 tasks\n",
      "layer1 head 24 processed\n",
      "Processed 25 /1024 tasks\n",
      "layer1 head 25 processed\n",
      "Processed 26 /1024 tasks\n",
      "layer1 head 26 processed\n",
      "Processed 27 /1024 tasks\n",
      "layer1 head 27 processed\n",
      "Processed 28 /1024 tasks\n",
      "layer1 head 28 processed\n",
      "Processed 29 /1024 tasks\n",
      "layer1 head 29 processed\n",
      "Processed 30 /1024 tasks\n",
      "layer1 head 30 processed\n",
      "Processed 31 /1024 tasks\n",
      "layer1 head 31 processed\n",
      "Processed 32 /1024 tasks\n",
      "layer2 head 0 processed\n",
      "Processed 1 /1024 tasks\n",
      "layer2 head 3 processed\n",
      "layer2 head 1 processed\n",
      "Processed 2 /1024 tasks\n",
      "layer2 head 2 processed\n",
      "Processed 3 /1024 tasks\n",
      "Processed 4 /1024 tasks\n",
      "layer2 head 4 processed\n",
      "Processed 5 /1024 tasks\n",
      "layer2 head 6 processed\n",
      "layer2 head 5 processed\n",
      "layer2 head 7 processed\n",
      "Processed 6 /1024 tasks\n",
      "Processed 7 /1024 tasks\n",
      "Processed 8 /1024 tasks\n",
      "layer2 head 8 processed\n",
      "layer2 head 9 processed\n",
      "Processed 9 /1024 tasks\n",
      "Processed 10 /1024 tasks\n",
      "layer2 head 11 processed\n",
      "layer2 head 10 processed\n",
      "Processed 11 /1024 tasks\n",
      "Processed 12 /1024 tasks\n",
      "layer2 head 12 processed\n",
      "Processed 13 /1024 tasks\n",
      "layer2 head 13 processed\n",
      "Processed 14 /1024 tasks\n",
      "layer2 head 14 processed\n",
      "Processed 15 /1024 tasks\n",
      "layer2 head 15 processed\n",
      "Processed 16 /1024 tasks\n",
      "layer2 head 16 processed\n",
      "Processed 17 /1024 tasks\n",
      "layer2 head 17 processed\n",
      "Processed 18 /1024 tasks\n",
      "layer2 head 19 processed\n",
      "layer2 head 18 processed\n",
      "Processed 19 /1024 tasks\n",
      "Processed 20 /1024 tasks\n",
      "layer2 head 20 processed\n",
      "Processed 21 /1024 tasks\n",
      "layer2 head 23 processed\n",
      "layer2 head 22 processed\n",
      "layer2 head 21 processed\n",
      "Processed 22 /1024 tasks\n",
      "Processed 23 /1024 tasks\n",
      "Processed 24 /1024 tasks\n",
      "layer2 head 24 processed\n",
      "Processed 25 /1024 tasks\n",
      "layer2 head 27 processed\n",
      "layer2 head 26 processed\n",
      "layer2 head 25 processed\n",
      "Processed 26 /1024 tasks\n",
      "Processed 27 /1024 tasks\n",
      "Processed 28 /1024 tasks\n",
      "layer2 head 30 processed\n",
      "layer2 head 28 processed\n",
      "Processed 29 /1024 tasks\n",
      "layer2 head 31 processed\n",
      "layer2 head 29 processed\n",
      "Processed 30 /1024 tasks\n",
      "Processed 31 /1024 tasks\n",
      "Processed 32 /1024 tasks\n",
      "layer3 head 1 processed\n",
      "layer3 head 3 processed\n",
      "layer3 head 0 processed\n",
      "Processed 1 /1024 tasks\n",
      "Processed 2 /1024 tasks\n",
      "layer3 head 2 processed\n",
      "Processed 3 /1024 tasks\n",
      "Processed 4 /1024 tasks\n",
      "layer3 head 4 processed\n",
      "Processed 5 /1024 tasks\n",
      "layer3 head 5 processed\n",
      "Processed 6 /1024 tasks\n",
      "layer3 head 6 processed\n",
      "layer3 head 7 processed\n",
      "Processed 7 /1024 tasks\n",
      "Processed 8 /1024 tasks\n",
      "layer3 head 9 processed\n",
      "layer3 head 8 processed\n",
      "Processed 9 /1024 tasks\n",
      "Processed 10 /1024 tasks\n",
      "layer3 head 10 processed\n",
      "Processed 11 /1024 tasks\n",
      "layer3 head 11 processed\n",
      "Processed 12 /1024 tasks\n",
      "layer3 head 12 processed\n",
      "Processed 13 /1024 tasks\n",
      "layer3 head 14 processed\n",
      "layer3 head 13 processed\n",
      "Processed 14 /1024 tasks\n",
      "Processed 15 /1024 tasks\n",
      "layer3 head 15 processed\n",
      "Processed 16 /1024 tasks\n",
      "layer3 head 16 processed\n",
      "Processed 17 /1024 tasks\n",
      "layer3 head 17 processed\n",
      "Processed 18 /1024 tasks\n",
      "layer3 head 19 processed\n",
      "layer3 head 18 processed\n",
      "Processed 19 /1024 tasks\n",
      "Processed 20 /1024 tasks\n",
      "layer3 head 20 processed\n",
      "Processed 21 /1024 tasks\n",
      "layer3 head 21 processed\n",
      "Processed 22 /1024 tasks\n",
      "layer3 head 22 processed\n",
      "Processed 23 /1024 tasks\n",
      "layer3 head 23 processed\n",
      "Processed 24 /1024 tasks\n",
      "layer3 head 25 processed\n",
      "layer3 head 24 processed\n",
      "Processed 25 /1024 tasks\n",
      "Processed 26 /1024 tasks\n",
      "layer3 head 26 processed\n",
      "Processed 27 /1024 tasks\n",
      "layer3 head 27 processed\n",
      "Processed 28 /1024 tasks\n",
      "layer3 head 28 processed\n",
      "Processed 29 /1024 tasks\n",
      "layer3 head 29 processed\n",
      "Processed 30 /1024 tasks\n",
      "layer3 head 30 processed\n",
      "Processed 31 /1024 tasks\n",
      "layer3 head 31 processed\n",
      "Processed 32 /1024 tasks\n",
      "layer4 head 2 processed\n",
      "layer4 head 0 processed\n",
      "layer4 head 3 processed\n",
      "Processed 1 /1024 tasks\n",
      "layer4 head 1 processed\n",
      "Processed 2 /1024 tasks\n",
      "Processed 3 /1024 tasks\n",
      "Processed 4 /1024 tasks\n",
      "layer4 head 4 processed\n",
      "Processed 5 /1024 tasks\n",
      "layer4 head 6 processed\n",
      "layer4 head 7 processed\n",
      "layer4 head 5 processed\n",
      "Processed 6 /1024 tasks\n",
      "Processed 7 /1024 tasks\n",
      "Processed 8 /1024 tasks\n",
      "layer4 head 8 processed\n",
      "Processed 9 /1024 tasks\n",
      "layer4 head 11 processed\n",
      "layer4 head 10 processed\n",
      "layer4 head 9 processed\n",
      "Processed 10 /1024 tasks\n",
      "Processed 11 /1024 tasks\n",
      "Processed 12 /1024 tasks\n",
      "layer4 head 12 processed\n",
      "Processed 13 /1024 tasks\n",
      "layer4 head 15 processed\n",
      "layer4 head 13 processed\n",
      "Processed 14 /1024 tasks\n",
      "layer4 head 14 processed\n",
      "Processed 15 /1024 tasks\n",
      "Processed 16 /1024 tasks\n",
      "layer4 head 16 processed\n",
      "Processed 17 /1024 tasks\n",
      "layer4 head 18 processed\n",
      "layer4 head 17 processed\n",
      "Processed 18 /1024 tasks\n",
      "Processed 19 /1024 tasks\n",
      "layer4 head 19 processed\n",
      "Processed 20 /1024 tasks\n",
      "layer4 head 20 processed\n",
      "Processed 21 /1024 tasks\n",
      "layer4 head 21 processed\n",
      "Processed 22 /1024 tasks\n",
      "layer4 head 22 processed\n",
      "Processed 23 /1024 tasks\n",
      "layer4 head 23 processed\n",
      "Processed 24 /1024 tasks\n",
      "layer4 head 24 processed\n",
      "Processed 25 /1024 tasks\n",
      "layer4 head 25 processed\n",
      "Processed 26 /1024 tasks\n",
      "layer4 head 26 processed\n",
      "Processed 27 /1024 tasks\n",
      "layer4 head 27 processed\n",
      "Processed 28 /1024 tasks\n",
      "layer4 head 28 processed\n",
      "Processed 29 /1024 tasks\n",
      "layer4 head 30 processed\n",
      "layer4 head 29 processed\n",
      "Processed 30 /1024 tasks\n",
      "Processed 31 /1024 tasks\n",
      "layer4 head 31 processed\n",
      "Processed 32 /1024 tasks\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# 导入clear_output\n",
    "# from IPython.display import clear_output\n",
    "# 定义处理函数\n",
    "def process_attention_head(args):\n",
    "    layer_idx, head_idx, q_matrixes, k_matrixes, attention_heads_list = args\n",
    "    # 提取对应层和头的q和k矩阵\n",
    "    q_matrix = q_matrixes[layer_idx, head_idx, :, :].reshape(128, 4096).to(device)\n",
    "    k_matrix = k_matrixes[layer_idx, head_idx, :, :].reshape(128, 4096).to(device)\n",
    "\n",
    "    # 计算q^Tk，注意这里的q和k都是已经reshape过的，所以需要转置k来进行矩阵乘法\n",
    "    q_k_product = torch.matmul(q_matrix.t(), k_matrix)\n",
    "\n",
    "    # 计算特征值和特征向量\n",
    "    eigenvalues, eigenvectors = torch.linalg.eig(q_k_product)\n",
    "    print(f'layer{layer_idx} head {head_idx} processed')\n",
    "    attention_heads_list[head_idx] = {\n",
    "        'eigenvalues': eigenvalues.to('cpu').detach().numpy(),\n",
    "        'eigenvectors': eigenvectors.permute(1,0).to('cpu').detach().numpy()\n",
    "    }\n",
    "    return \n",
    "\n",
    "def run_qk(layer_idx,attention_heads_list_one_layer):\n",
    "    count = 0\n",
    "    # 创建 ThreadPoolExecutor，并指定线程数\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # 提交任务到线程池\n",
    "        futures = []\n",
    "        for head_idx in range(32):\n",
    "            args = (layer_idx, head_idx, q_matrixes, k_matrixes, attention_heads_list_one_layer)\n",
    "            future = executor.submit(process_attention_head, args)\n",
    "            futures.append(future)\n",
    "        \n",
    "        # 等待所有任务完成\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "            count += 1\n",
    "            # clear_output(wait=True)\n",
    "            print(f\"Processed {count} /1024 tasks\")\n",
    "\n",
    "\n",
    "# 在此之前需要定义q_matrixes, k_matrixes 和 device\n",
    "# 构建32*32的列表，对应32层下的32个注意力头\n",
    "for layer_idx in range(0,5):\n",
    "    attention_heads_list = np.array([{} for _ in range(32)])\n",
    "    run_qk(layer_idx, attention_heads_list)\n",
    "    with open(f'datas/QK_datas/QK_arraylayer_{layer_idx}.npy', 'wb') as f:\n",
    "        np.save(f, attention_heads_list)\n",
    "    del attention_heads_list\n",
    "    # attention_heads_list = np.array([[{} for _ in range(32)] for _ in range(32)])\n",
    "# # 运行函数\n",
    "# run_qk()\n",
    "# # 保存array变量attention_heads_list到文件中\n",
    "# with open('datas/QK_array.npy', 'wb') as f:\n",
    "#     np.save(f, attention_heads_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ov矩阵的特征值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4096) must match the existing size (55296) at non-singleton dimension 1.  Target sizes: [128, 4096].  Tensor sizes: [128, 55296]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m     68\u001b[0m     attention_heads_list_ov \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m32\u001b[39m)])\n\u001b[1;32m---> 69\u001b[0m     run_ov(layer_idx, attention_heads_list_ov)\n\u001b[0;32m     70\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatas/OV_datas/OV_arraylayer_\u001b[39m\u001b[39m{\u001b[39;00mlayer_idx\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     71\u001b[0m         np\u001b[39m.\u001b[39msave(f, attention_heads_list_ov)\n",
      "Cell \u001b[1;32mIn[15], line 59\u001b[0m, in \u001b[0;36mrun_ov\u001b[1;34m(layer_idx, attention_heads_list_one_layer)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# 等待所有任务完成\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m futures:\n\u001b[1;32m---> 59\u001b[0m     future\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m     60\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     61\u001b[0m     \u001b[39m# clear_output(wait=True)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm2-6b\\lib\\concurrent\\futures\\_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm2-6b\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm2-6b\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mprocess_attention_head_ov\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     32\u001b[0m     eovu_subproduct \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(e_sub, ov_product)\n\u001b[0;32m     33\u001b[0m     eovu_subproduct \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(eovu_subproduct, u_matrix\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mt())\n\u001b[1;32m---> 34\u001b[0m     eovu_product[i:end_idx] \u001b[39m=\u001b[39m eovu_subproduct\n\u001b[0;32m     38\u001b[0m \u001b[39m# 计算特征值和特征向量\u001b[39;00m\n\u001b[0;32m     39\u001b[0m eigenvalues \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigvals(eovu_product\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (4096) must match the existing size (55296) at non-singleton dimension 1.  Target sizes: [128, 4096].  Tensor sizes: [128, 55296]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def process_attention_head_ov(args):\n",
    "    layer_idx, head_idx, o_matrixes, v_matrixes, attention_heads_list_ov = args\n",
    "    # 提取对应层和头的q和k矩阵\n",
    "    o_matrix = o_matrixes[layer_idx, head_idx, :, :].reshape(128, 4096).to(device)\n",
    "    v_matrix = v_matrixes[layer_idx, head_idx, :, :].reshape(4096, 128).to(device)\n",
    "    \n",
    "    # 计算O*V，注意这里的q和k都是已经reshape过的，所以需要转置k来进行矩阵乘法\n",
    "    eovu_product = torch.matmul(e_matrix.float().to(device), torch.matmul(torch.matmul(o_matrix.t(), v_matrix.t()), u_matrix.float().to(device).t()))\n",
    "    # eovu_product = torch.matmul(o_matrix.t(), v_matrix.t())\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算特征值和特征向量\n",
    "    eigenvalues = torch.linalg.eigvals(eovu_product.float())\n",
    "    print(f'layer{layer_idx} head {head_idx} processed')\n",
    "    attention_heads_list_ov[head_idx] = {\n",
    "        'eigenvalues': eigenvalues.to('cpu').detach().numpy(),\n",
    "    }\n",
    "    return\n",
    "\n",
    "def run_ov(layer_idx,attention_heads_list_one_layer):\n",
    "    count = 0\n",
    "    # 创建 ThreadPoolExecutor，并指定线程数\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # 提交任务到线程池\n",
    "        futures = []\n",
    "        for head_idx in range(32):\n",
    "            args = (layer_idx, head_idx, o_matrixes, v_matrixes, attention_heads_list_one_layer)\n",
    "            future = executor.submit(process_attention_head_ov, args)\n",
    "            futures.append(future)\n",
    "        \n",
    "        # 等待所有任务完成\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "            count += 1\n",
    "            # clear_output(wait=True)\n",
    "            print(f\"Processed {count} /1024 tasks\")\n",
    "\n",
    "\n",
    "# 在此之前需要定义q_matrixes, k_matrixes 和 device\n",
    "# 构建32*32的列表，对应32层下的32个注意力头\n",
    "for layer_idx in range(1):\n",
    "    attention_heads_list_ov = np.array([{} for _ in range(32)])\n",
    "    run_ov(layer_idx, attention_heads_list_ov)\n",
    "    with open(f'datas/OV_datas/OV_arraylayer_{layer_idx}.npy', 'wb') as f:\n",
    "        np.save(f, attention_heads_list_ov)\n",
    "    del attention_heads_list_ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存array变量attention_heads_list到文件中\n",
    "with open('datas/QK_array.npy', 'wb') as f:\n",
    "    np.save(f, attention_heads_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_q_matrix = q_matrixes[0, 0, :, :].reshape(128, 4096)\n",
    "tmp_k_matrix = k_matrixes[0, 0, :, :].reshape(128, 4096)\n",
    "tq = tmp_q_matrix.clone().detach()\n",
    "tk = tmp_q_matrix.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.matmul(tq.t(),tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta,tb = torch.linalg.eig(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 取第一个特征值和特征向量\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m first_eigenvalue \u001b[39m=\u001b[39m ta[:, \u001b[39m0\u001b[39;49m]\n\u001b[1;32m      3\u001b[0m first_eigenvector \u001b[39m=\u001b[39m tb[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39m# 验证 Ax = λx\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# 取第一个特征值和特征向量\n",
    "first_eigenvalue = ta[:, 0]\n",
    "first_eigenvector = tb[:, 0]\n",
    "\n",
    "# 验证 Ax = λx\n",
    "Ax = torch.matmul(mat, first_eigenvector)\n",
    "Ax_over_lambda = Ax / first_eigenvalue\n",
    "\n",
    "# 打印验证结果\n",
    "print(\"验证结果:\", torch.allclose(Ax_over_lambda, first_eigenvector))\n",
    "\n",
    "# 打印特征值和特征向量\n",
    "print(\"第一个特征值:\", first_eigenvalue)\n",
    "print(\"第一个特征向量:\", first_eigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = ta[0]*tb[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0082+0.j, -0.0201+0.j,  0.0175+0.j,  ...,  0.0108+0.j,  0.0203+0.j,\n",
       "        -0.0167+0.j])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-14.1930+0.j, -34.7486+0.j,  30.3795+0.j,  ...,  18.7687+0.j,  35.1911+0.j,\n",
       "        -28.8638+0.j])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-14.1924, -34.7503,  30.3795,  ...,  18.7687,  35.1912, -28.8638])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = torch.matmul(mat,tb[:,0].real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "print(mat.shape)\n",
    "print(tb[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
